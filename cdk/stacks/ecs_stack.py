from aws_cdk import (
    Stack,
    Duration,
    aws_ecs as ecs,
    aws_ecs_patterns as ecs_patterns,
    aws_iam as iam,
    aws_logs as logs,
    aws_ec2 as ec2,
    CfnOutput
)
from constructs import Construct

class EcsStack(Stack):
    def __init__(self, scope: Construct, construct_id: str, vpc, secrets, knowledge_base_id, data_bucket, opensearch_endpoint, vector_lambda_arn, **kwargs) -> None:
        super().__init__(scope, construct_id, **kwargs)

        # ECS Cluster
        cluster = ecs.Cluster(
            self, "NotionChatbotCluster",
            vpc=vpc,
            cluster_name="notion-chatbot-cluster",
            enable_fargate_capacity_providers=True
        )

        # Task Role - Bedrock ë° Secrets Manager ì ‘ê·¼ ê¶Œí•œ
        task_role = iam.Role(
            self, "NotionChatbotTaskRole",
            assumed_by=iam.ServicePrincipal("ecs-tasks.amazonaws.com"),
            managed_policies=[
                iam.ManagedPolicy.from_aws_managed_policy_name("service-role/AmazonECSTaskExecutionRolePolicy")
            ]
        )

        # S3 ì ‘ê·¼ ê¶Œí•œ ì¶”ê°€
        data_bucket.grant_read(task_role)
        task_role.add_to_policy(
            iam.PolicyStatement(
                effect=iam.Effect.ALLOW,
                actions=[
                    "s3:ListBucket"
                ],
                resources=[data_bucket.bucket_arn]
            )
        )

        # OpenSearch ì ‘ê·¼ ê¶Œí•œ ì¶”ê°€
        task_role.add_to_policy(
            iam.PolicyStatement(
                effect=iam.Effect.ALLOW,
                actions=[
                    "aoss:APIAccessAll"
                ],
                resources=["*"]
            )
        )

        # Bedrock Titan Embedding ëª¨ë¸ ì ‘ê·¼ ê¶Œí•œ ì¶”ê°€
        task_role.add_to_policy(
            iam.PolicyStatement(
                effect=iam.Effect.ALLOW,
                actions=[
                    "bedrock:InvokeModel"
                ],
                resources=[
                    f"arn:aws:bedrock:{self.region}::foundation-model/amazon.titan-embed-text-v2:0"
                ]
            )
        )

        # Lambda í˜¸ì¶œ ê¶Œí•œ ì¶”ê°€ (ê¸°ì¡´ + ë²¡í„° Lambda)
        task_role.add_to_policy(
            iam.PolicyStatement(
                effect=iam.Effect.ALLOW,
                actions=[
                    "lambda:InvokeFunction"
                ],
                resources=[
                    f"arn:aws:lambda:{self.region}:{self.account}:function:NotionChatbotBedrockStack-NotionSyncFunction*",
                    vector_lambda_arn
                ]
            )
        )

        # Bedrock ì ‘ê·¼ ê¶Œí•œ ì¶”ê°€
        task_role.add_to_policy(
            iam.PolicyStatement(
                effect=iam.Effect.ALLOW,
                actions=[
                    "bedrock:InvokeModel",
                    "bedrock:InvokeModelWithResponseStream",
                    "bedrock:Retrieve",
                    "bedrock:RetrieveAndGenerate"
                ],
                resources=[
                    f"arn:aws:bedrock:{self.region}::foundation-model/anthropic.claude-3-haiku-20240307-v1:0",
                    f"arn:aws:bedrock:{self.region}:{self.account}:knowledge-base/{knowledge_base_id}"
                ]
            )
        )

        # Secrets Manager ì ‘ê·¼ ê¶Œí•œ ì¶”ê°€
        for secret in secrets.values():
            secret.grant_read(task_role)

        # Task Definition
        task_definition = ecs.FargateTaskDefinition(
            self, "NotionChatbotTaskDef",
            memory_limit_mib=1024,
            cpu=512,
            task_role=task_role
        )

        # Container Definition
        container = task_definition.add_container(
            "NotionChatbotContainer",
            image=ecs.ContainerImage.from_registry("python:3.11-slim"),
            logging=ecs.LogDrivers.aws_logs(
                stream_prefix="notion-chatbot",
                log_group=logs.LogGroup(
                    self, "NotionChatbotLogGroup",
                    log_group_name="/ecs/notion-chatbot",
                    retention=logs.RetentionDays.ONE_WEEK
                )
            ),
            environment={
                "AWS_DEFAULT_REGION": self.region,
                "STREAMLIT_SERVER_PORT": "8501",
                "STREAMLIT_SERVER_ADDRESS": "0.0.0.0",
                "NOTION_TOKEN": "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",
                "KNOWLEDGE_BASE_ID": knowledge_base_id,
                "OPENSEARCH_ENDPOINT": opensearch_endpoint,
                "VECTOR_LAMBDA_ARN": vector_lambda_arn
            },
            secrets={
                "NOTION_TOKEN_SECRET_ARN": ecs.Secret.from_secrets_manager(secrets["notion_token"]),
                "APP_CONFIG_SECRET_ARN": ecs.Secret.from_secrets_manager(secrets["app_config"])
            },
            command=[
                "bash", "-c", 
                """
                pip install streamlit boto3 requests notion-client opensearch-py &&
                mkdir -p /app && cd /app &&
                cat > app.py << 'STREAMLIT_EOF'
import streamlit as st
import boto3
import json
import os
import requests
from datetime import datetime
from typing import List, Dict, Any

st.set_page_config(page_title='ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”! ğŸ¤–', page_icon='ğŸ¤–', layout='wide')

st.markdown('<div style="text-align: center; padding: 1rem 0; background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); color: white; border-radius: 10px; margin-bottom: 2rem;"><h1>ğŸ¤– ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!</h1><p>Notion ì§€ì‹ ê¸°ë°˜ì—ì„œ ë‹µë³€ì„ ì°¾ì•„ë“œë¦½ë‹ˆë‹¤ (ê²€ìƒ‰ ë°©ì‹ ë¹„êµ ë°ëª¨)</p></div>', unsafe_allow_html=True)

# AWS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
@st.cache_resource
def get_aws_clients():
    s3 = boto3.client('s3', region_name='ap-northeast-2')
    bedrock = boto3.client('bedrock-runtime', region_name='ap-northeast-2')
    lambda_client = boto3.client('lambda', region_name='ap-northeast-2')
    return s3, bedrock, lambda_client

s3_client, bedrock_client, lambda_client = get_aws_clients()
knowledge_base_id = os.getenv('KNOWLEDGE_BASE_ID', 'simple-kb-demo')
opensearch_endpoint = os.getenv('OPENSEARCH_ENDPOINT', '')
vector_lambda_arn = os.getenv('VECTOR_LAMBDA_ARN', '')

class OpenSearchClient:
    def __init__(self, endpoint: str):
        self.endpoint = endpoint
        self.bedrock_client = boto3.client('bedrock-runtime', region_name='ap-northeast-2')
    
    def generate_embedding(self, text: str) -> List[float]:
        try:
            response = self.bedrock_client.invoke_model(
                modelId='amazon.titan-embed-text-v2:0',
                body=json.dumps({'inputText': text[:8000]})
            )
            response_body = json.loads(response['body'].read())
            return response_body['embedding']
        except Exception as e:
            st.error(f'ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {str(e)}')
            return []
    
    def semantic_search(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        try:
            query_embedding = self.generate_embedding(query)
            if not query_embedding:
                return []
            
            search_body = {
                'size': limit,
                'query': {
                    'knn': {
                        'embedding': {
                            'vector': query_embedding,
                            'k': limit
                        }
                    }
                },
                '_source': ['id', 'title', 'content', 'url', 'metadata']
            }
            
            response = requests.post(
                f'{self.endpoint}/notion-index/_search',
                json=search_body,
                headers={'Content-Type': 'application/json'}
            )
            
            if response.status_code == 200:
                results = response.json()
                documents = []
                for hit in results.get('hits', {}).get('hits', []):
                    source = hit['_source']
                    documents.append({
                        'id': source.get('id'),
                        'title': source.get('title', 'ì œëª© ì—†ìŒ'),
                        'content': source.get('content', ''),
                        'url': source.get('url', ''),
                        'similarity_score': hit['_score']
                    })
                return documents
            return []
        except Exception as e:
            st.error(f'ì˜ë¯¸ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}')
            return []

# OpenSearch í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
opensearch_client = None
if opensearch_endpoint:
    opensearch_client = OpenSearchClient(opensearch_endpoint)

def search_opensearch_documents(query, search_method='semantic'):
    try:
        if not opensearch_client:
            st.error('OpenSearchê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
            return []
        
        documents = opensearch_client.semantic_search(query, limit=5)
        return documents
        
    except Exception as e:
        st.error(f'OpenSearch ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}')
        return []

def search_s3_documents(query, bucket_name):
    try:
        response = s3_client.list_objects_v2(
            Bucket=bucket_name,
            Prefix='notion-data/',
            MaxKeys=50
        )
        
        documents = []
        
        if 'Contents' in response:
            for obj in response['Contents']:
                if obj['Key'].endswith('.json'):
                    doc_response = s3_client.get_object(
                        Bucket=bucket_name,
                        Key=obj['Key']
                    )
                    doc_content = json.loads(doc_response['Body'].read())
                    
                    query_lower = query.lower()
                    title_lower = doc_content.get('title', '').lower()
                    content_lower = doc_content.get('content', '').lower()
                    
                    if any(word in title_lower or word in content_lower for word in query_lower.split()):
                        documents.append(doc_content)
        
        return documents[:5]
        
    except Exception as e:
        st.error(f'S3 ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}')
        return []

def generate_bedrock_response(query, documents):
    try:
        context_parts = []
        for doc in documents:
            title = doc.get('title', 'ì œëª© ì—†ìŒ')
            content = doc.get('content', '')[:500]
            context_parts.append(f'ë¬¸ì„œ: {title}\\në‚´ìš©: {content}')
        
        context = '\\n\\n'.join(context_parts)
        
        prompt = f'''ì‚¬ìš©ì ì§ˆë¬¸: {query}

ê´€ë ¨ Notion ë¬¸ì„œë“¤:
{context}

ìœ„ì˜ Notion ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”.'''
        
        body = {
            'anthropic_version': 'bedrock-2023-05-31',
            'max_tokens': 2000,
            'temperature': 0.1,
            'messages': [{'role': 'user', 'content': prompt}]
        }
        
        response = bedrock_client.invoke_model(
            modelId='anthropic.claude-3-haiku-20240307-v1:0',
            body=json.dumps(body)
        )
        
        response_body = json.loads(response['body'].read())
        return response_body['content'][0]['text']
        
    except Exception as e:
        return f'ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'

# ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
if 'messages' not in st.session_state:
    st.session_state.messages = []

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.markdown('## âš™ï¸ ì„¤ì •')
    
    # ê²€ìƒ‰ ë°©ì‹ ì„ íƒ
    st.markdown('### ğŸ” ê²€ìƒ‰ ë°©ì‹ ì„ íƒ')
    search_method = st.selectbox(
        'ê²€ìƒ‰ ë°©ì‹ì„ ì„ íƒí•˜ì„¸ìš”:',
        ['S3 í‚¤ì›Œë“œ ê²€ìƒ‰', 'OpenSearch ì˜ë¯¸ ê²€ìƒ‰'],
        help='S3: í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜, OpenSearch: ì˜ë¯¸ ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰'
    )
    
    # ê²€ìƒ‰ ë°©ì‹ë³„ ì„¤ëª…
    if search_method == 'S3 í‚¤ì›Œë“œ ê²€ìƒ‰':
        st.info('ğŸ“ í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ ê²€ìƒ‰\\n- ë¹ ë¥¸ ì†ë„\\n- ì •í™•í•œ í‚¤ì›Œë“œ ì¼ì¹˜\\n- ë¹„ìš© íš¨ìœ¨ì ')
    else:
        st.info('ğŸ§  ì˜ë¯¸ ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰\\n- ë¬¸ë§¥ ì´í•´\\n- ìœ ì‚¬í•œ ì˜ë¯¸ ê²€ìƒ‰\\n- ë†’ì€ ì •í™•ë„')
    
    # Knowledge Base ì •ë³´
    st.markdown('### ğŸ§  Knowledge Base')
    if search_method == 'S3 í‚¤ì›Œë“œ ê²€ìƒ‰':
        st.success(f'S3 ê¸°ë°˜ ê²€ìƒ‰: {knowledge_base_id}')
    else:
        if opensearch_endpoint:
            st.success('OpenSearch Serverless ì—°ê²°ë¨')
        else:
            st.warning('OpenSearch ì„¤ì • í•„ìš”')
    
    if st.button('ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”'):
        st.session_state.messages = []
        st.rerun()

# ì´ì „ ë©”ì‹œì§€ë“¤ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message['role']):
        st.markdown(message['content'])
        if message.get('sources'):
            with st.expander('ğŸ“š ì°¸ê³  ë¬¸ì„œ'):
                for i, source in enumerate(message['sources'], 1):
                    st.markdown(f'**{i}. {source.get("title", "ë¬¸ì„œ")}**')
                    content_preview = source.get("content", "ë‚´ìš© ì—†ìŒ")[:200]
                    st.markdown(f'ë‚´ìš©: {content_preview}...')
                    if source.get('url'):
                        st.markdown(f'[ğŸ“„ ì›ë³¸ ë³´ê¸°]({source["url"]})')
                    if source.get('similarity_score'):
                        st.markdown(f'ìœ ì‚¬ë„ ì ìˆ˜: {source["similarity_score"]:.3f}')
                    st.markdown('---')

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input('ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”! ì˜ˆ: í”„ë¡œì íŠ¸ ì¼ì •ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?'):
    st.session_state.messages.append({'role': 'user', 'content': prompt})
    
    with st.chat_message('user'):
        st.markdown(prompt)
    
    with st.chat_message('assistant'):
        with st.spinner('ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤... ğŸ¤”'):
            try:
                bucket_name = f'notion-chatbot-data-965037532757-ap-northeast-2'
                
                # ì„ íƒëœ ê²€ìƒ‰ ë°©ì‹ì— ë”°ë¼ ë¬¸ì„œ ê²€ìƒ‰
                if search_method == 'S3 í‚¤ì›Œë“œ ê²€ìƒ‰':
                    documents = search_s3_documents(prompt, bucket_name)
                    search_info = f'ğŸ” S3 í‚¤ì›Œë“œ ê²€ìƒ‰ ì‚¬ìš©'
                else:
                    documents = search_opensearch_documents(prompt)
                    search_info = f'ğŸ§  OpenSearch ì˜ë¯¸ ê²€ìƒ‰ ì‚¬ìš©'
                
                st.info(search_info)
                
                if documents:
                    # Bedrockìœ¼ë¡œ RAG ì‘ë‹µ ìƒì„±
                    answer = generate_bedrock_response(prompt, documents)
                    st.markdown(answer)
                    
                    # ì°¸ê³  ë¬¸ì„œ í‘œì‹œ
                    with st.expander('ğŸ“š ì°¸ê³  ë¬¸ì„œ', expanded=True):
                        for i, doc in enumerate(documents[:3], 1):
                            st.markdown(f'**{i}. {doc.get("title", "ë¬¸ì„œ")}**')
                            content_preview = doc.get('content', 'ë‚´ìš© ì—†ìŒ')[:200]
                            st.markdown(f'ë‚´ìš©: {content_preview}...')
                            if doc.get('url'):
                                st.markdown(f'[ğŸ“„ ì›ë³¸ ë³´ê¸°]({doc["url"]})')
                            if doc.get('similarity_score'):
                                st.markdown(f'ìœ ì‚¬ë„ ì ìˆ˜: {doc["similarity_score"]:.3f}')
                            st.markdown('---')
                    
                    # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                    st.session_state.messages.append({
                        'role': 'assistant', 
                        'content': answer,
                        'sources': documents
                    })
                else:
                    no_result_msg = 'ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Notion ë°ì´í„°ê°€ ë™ê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.'
                    st.markdown(no_result_msg)
                    st.session_state.messages.append({'role': 'assistant', 'content': no_result_msg})
                    
            except Exception as e:
                error_msg = f'ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
                st.error(error_msg)
                st.session_state.messages.append({'role': 'assistant', 'content': error_msg})

st.markdown('---')
st.markdown('<div style="text-align: center; color: #666; padding: 1rem;"><p>ğŸ” S3 í‚¤ì›Œë“œ ê²€ìƒ‰ vs ğŸ§  OpenSearch ì˜ë¯¸ ê²€ìƒ‰ ë¹„êµ ë°ëª¨</p><p>ğŸ“š ë‘ ê°€ì§€ ê²€ìƒ‰ ë°©ì‹ì˜ ì°¨ì´ë¥¼ ì§ì ‘ ì²´í—˜í•´ë³´ì„¸ìš”</p><p>ğŸ”„ 1ì‹œê°„ë§ˆë‹¤ Notion ë°ì´í„° ìë™ ë™ê¸°í™”</p></div>', unsafe_allow_html=True)
STREAMLIT_EOF
                streamlit run app.py --server.port=8501 --server.address=0.0.0.0
                """
            ]
        )

        # Port Mapping
        container.add_port_mappings(
            ecs.PortMapping(
                container_port=8501,
                protocol=ecs.Protocol.TCP
            )
        )

        # Application Load Balanced Fargate Service
        self.fargate_service = ecs_patterns.ApplicationLoadBalancedFargateService(
            self, "NotionChatbotService",
            cluster=cluster,
            task_definition=task_definition,
            public_load_balancer=True,
            listener_port=80,
            desired_count=1,  # ë¹„ìš© ì ˆì•½ì„ ìœ„í•´ 1ê°œ ì¸ìŠ¤í„´ìŠ¤
            platform_version=ecs.FargatePlatformVersion.LATEST,
            assign_public_ip=True,
            service_name="notion-chatbot-service"
        )

        # Health Check ì„¤ì •
        self.fargate_service.target_group.configure_health_check(
            path="/",
            healthy_http_codes="200",
            timeout=Duration.seconds(10),
            interval=Duration.seconds(30),
            healthy_threshold_count=2,
            unhealthy_threshold_count=5
        )

        # Auto Scaling ì„¤ì • (ì„ íƒì‚¬í•­)
        scaling = self.fargate_service.service.auto_scale_task_count(
            min_capacity=1,
            max_capacity=3
        )

        scaling.scale_on_cpu_utilization(
            "CpuScaling",
            target_utilization_percent=70,
            scale_in_cooldown=Duration.minutes(5),
            scale_out_cooldown=Duration.minutes(2)
        )

        # Outputs
        CfnOutput(
            self, "LoadBalancerDNS",
            value=self.fargate_service.load_balancer.load_balancer_dns_name,
            description="Load Balancer DNS Name"
        )

        CfnOutput(
            self, "ServiceURL",
            value=f"http://{self.fargate_service.load_balancer.load_balancer_dns_name}",
            description="Notion Chatbot Service URL"
        )
