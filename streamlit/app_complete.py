import streamlit as st
import boto3
import json
import os
import requests
from datetime import datetime
from typing import List, Dict, Any

st.set_page_config(page_title='ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”! ğŸ¤–', page_icon='ğŸ¤–', layout='wide')

st.markdown('<div style="text-align: center; padding: 1rem 0; background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); color: white; border-radius: 10px; margin-bottom: 2rem;"><h1>ğŸ¤– ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!</h1><p>Notion ì§€ì‹ ê¸°ë°˜ì—ì„œ ë‹µë³€ì„ ì°¾ì•„ë“œë¦½ë‹ˆë‹¤ (ê²€ìƒ‰ ë°©ì‹ ë¹„êµ ë°ëª¨)</p></div>', unsafe_allow_html=True)

# AWS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
@st.cache_resource
def get_aws_clients():
    s3 = boto3.client('s3', region_name='ap-northeast-2')
    bedrock = boto3.client('bedrock-runtime', region_name='ap-northeast-2')
    lambda_client = boto3.client('lambda', region_name='ap-northeast-2')
    return s3, bedrock, lambda_client

s3_client, bedrock_client, lambda_client = get_aws_clients()
knowledge_base_id = os.getenv('KNOWLEDGE_BASE_ID', 'simple-kb-demo')
opensearch_endpoint = os.getenv('OPENSEARCH_ENDPOINT', '')
vector_lambda_arn = os.getenv('VECTOR_LAMBDA_ARN', '')

class OpenSearchClient:
    def __init__(self, endpoint: str):
        self.endpoint = endpoint
        self.bedrock_client = boto3.client('bedrock-runtime', region_name='ap-northeast-2')
    
    def generate_embedding(self, text: str) -> List[float]:
        try:
            response = self.bedrock_client.invoke_model(
                modelId='amazon.titan-embed-text-v1',
                body=json.dumps({'inputText': text[:8000]})
            )
            response_body = json.loads(response['body'].read())
            return response_body['embedding']
        except Exception as e:
            st.error(f'ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {str(e)}')
            return []
    
    def semantic_search(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        try:
            query_embedding = self.generate_embedding(query)
            if not query_embedding:
                return []
            
            search_body = {
                'size': limit,
                'query': {
                    'knn': {
                        'embedding': {
                            'vector': query_embedding,
                            'k': limit
                        }
                    }
                },
                '_source': ['id', 'title', 'content', 'url', 'metadata']
            }
            
            response = requests.post(
                f'{self.endpoint}/notion-index/_search',
                json=search_body,
                headers={'Content-Type': 'application/json'}
            )
            
            if response.status_code == 200:
                results = response.json()
                documents = []
                for hit in results.get('hits', {}).get('hits', []):
                    source = hit['_source']
                    documents.append({
                        'id': source.get('id'),
                        'title': source.get('title', 'ì œëª© ì—†ìŒ'),
                        'content': source.get('content', ''),
                        'url': source.get('url', ''),
                        'similarity_score': hit['_score']
                    })
                return documents
            return []
        except Exception as e:
            st.error(f'ì˜ë¯¸ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}')
            return []

# OpenSearch í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
opensearch_client = None
if opensearch_endpoint:
    opensearch_client = OpenSearchClient(opensearch_endpoint)

def search_opensearch_documents(query, search_method='semantic'):
    '''OpenSearchë¥¼ ì‚¬ìš©í•œ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰'''
    try:
        if not opensearch_client:
            st.error('OpenSearchê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
            return []
        
        documents = opensearch_client.semantic_search(query, limit=5)
        return documents
        
    except Exception as e:
        st.error(f'OpenSearch ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}')
        return []

def search_s3_documents(query, bucket_name):
    '''S3ì—ì„œ Notion ë¬¸ì„œ ê²€ìƒ‰'''
    try:
        response = s3_client.list_objects_v2(
            Bucket=bucket_name,
            Prefix='notion-data/',
            MaxKeys=50
        )
        
        documents = []
        
        if 'Contents' in response:
            for obj in response['Contents']:
                if obj['Key'].endswith('.json'):
                    doc_response = s3_client.get_object(
                        Bucket=bucket_name,
                        Key=obj['Key']
                    )
                    doc_content = json.loads(doc_response['Body'].read())
                    
                    query_lower = query.lower()
                    title_lower = doc_content.get('title', '').lower()
                    content_lower = doc_content.get('content', '').lower()
                    
                    if any(word in title_lower or word in content_lower for word in query_lower.split()):
                        documents.append(doc_content)
        
        return documents[:5]
        
    except Exception as e:
        st.error(f'S3 ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}')
        return []

def generate_bedrock_response(query, documents):
    '''Bedrockìœ¼ë¡œ RAG ì‘ë‹µ ìƒì„±'''
    try:
        context_parts = []
        for doc in documents:
            title = doc.get('title', 'ì œëª© ì—†ìŒ')
            content = doc.get('content', '')[:500]
            context_parts.append(f'ë¬¸ì„œ: {title}\në‚´ìš©: {content}')
        
        context = '\n\n'.join(context_parts)
        
        prompt = f'''ì‚¬ìš©ì ì§ˆë¬¸: {query}

ê´€ë ¨ Notion ë¬¸ì„œë“¤:
{context}

ìœ„ì˜ Notion ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”. ë‹µë³€í•  ë•ŒëŠ” ë‹¤ìŒ ì‚¬í•­ì„ ì§€ì¼œì£¼ì„¸ìš”:
1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”
2. í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
3. ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
4. ê°€ëŠ¥í•˜ë©´ êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ì„¸ë¶€ì‚¬í•­ì„ í¬í•¨í•˜ì„¸ìš”'''
        
        body = {
            'anthropic_version': 'bedrock-2023-05-31',
            'max_tokens': 2000,
            'temperature': 0.1,
            'messages': [{'role': 'user', 'content': prompt}]
        }
        
        response = bedrock_client.invoke_model(
            modelId='anthropic.claude-3-haiku-20240307-v1:0',
            body=json.dumps(body)
        )
        
        response_body = json.loads(response['body'].read())
        return response_body['content'][0]['text']
        
    except Exception as e:
        return f'ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'

# ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
if 'messages' not in st.session_state:
    st.session_state.messages = []

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.markdown('## âš™ï¸ ì„¤ì •')
    
    # ê²€ìƒ‰ ë°©ì‹ ì„ íƒ
    st.markdown('### ğŸ” ê²€ìƒ‰ ë°©ì‹ ì„ íƒ')
    search_method = st.selectbox(
        'ê²€ìƒ‰ ë°©ì‹ì„ ì„ íƒí•˜ì„¸ìš”:',
        ['S3 í‚¤ì›Œë“œ ê²€ìƒ‰', 'OpenSearch ì˜ë¯¸ ê²€ìƒ‰'],
        help='S3: í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜, OpenSearch: ì˜ë¯¸ ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰'
    )
    
    # ê²€ìƒ‰ ë°©ì‹ë³„ ì„¤ëª…
    if search_method == 'S3 í‚¤ì›Œë“œ ê²€ìƒ‰':
        st.info('ğŸ“ í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ ê²€ìƒ‰\n- ë¹ ë¥¸ ì†ë„\n- ì •í™•í•œ í‚¤ì›Œë“œ ì¼ì¹˜\n- ë¹„ìš© íš¨ìœ¨ì ')
    else:
        st.info('ğŸ§  ì˜ë¯¸ ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰\n- ë¬¸ë§¥ ì´í•´\n- ìœ ì‚¬í•œ ì˜ë¯¸ ê²€ìƒ‰\n- ë†’ì€ ì •í™•ë„')
    
    # Knowledge Base ì •ë³´
    st.markdown('### ğŸ§  Knowledge Base')
    if search_method == 'S3 í‚¤ì›Œë“œ ê²€ìƒ‰':
        st.success(f'S3 ê¸°ë°˜ ê²€ìƒ‰: {knowledge_base_id}')
    else:
        if opensearch_endpoint:
            st.success('OpenSearch Serverless ì—°ê²°ë¨')
        else:
            st.warning('OpenSearch ì„¤ì • í•„ìš”')
    
    if st.button('ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”'):
        st.session_state.messages = []
        st.rerun()
    
    if st.button('ğŸ”„ ë°ì´í„° ë™ê¸°í™” ì‹¤í–‰'):
        sync_type = st.radio('ë™ê¸°í™” ë°©ì‹:', ['S3ë§Œ ë™ê¸°í™”', 'S3 + OpenSearch ë™ê¸°í™”'])
        
        if st.button('ë™ê¸°í™” ì‹œì‘'):
            try:
                # S3 ë™ê¸°í™”
                response = lambda_client.invoke(
                    FunctionName='NotionChatbotBedrockStack-NotionSyncFunctionFFED61-DntTQBnmfaiG',
                    InvocationType='Event'
                )
                st.success('S3 ë™ê¸°í™” ì‘ì—…ì„ ì‹œì‘í–ˆìŠµë‹ˆë‹¤!')
                
                # OpenSearch ë™ê¸°í™” (ì„ íƒëœ ê²½ìš°)
                if sync_type == 'S3 + OpenSearch ë™ê¸°í™”' and vector_lambda_arn:
                    vector_response = lambda_client.invoke(
                        FunctionName=vector_lambda_arn.split(':')[-1],
                        InvocationType='Event'
                    )
                    st.success('OpenSearch ë²¡í„° ì¸ë±ì‹± ì‘ì—…ë„ ì‹œì‘í–ˆìŠµë‹ˆë‹¤!')
                    
            except Exception as e:
                st.error(f'ë™ê¸°í™” ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}')

# ì´ì „ ë©”ì‹œì§€ë“¤ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message['role']):
        st.markdown(message['content'])
        if message.get('sources'):
            with st.expander('ğŸ“š ì°¸ê³  ë¬¸ì„œ'):
                for i, source in enumerate(message['sources'], 1):
                    st.markdown(f'**{i}. {source.get("title", "ë¬¸ì„œ")}**')
                    content_preview = source.get("content", "ë‚´ìš© ì—†ìŒ")[:200]
                    st.markdown(f'ë‚´ìš©: {content_preview}...')
                    if source.get('url'):
                        st.markdown(f'[ğŸ“„ ì›ë³¸ ë³´ê¸°]({source["url"]})')
                    if source.get('similarity_score'):
                        st.markdown(f'ìœ ì‚¬ë„ ì ìˆ˜: {source["similarity_score"]:.3f}')
                    st.markdown('---')

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input('ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”! ì˜ˆ: í”„ë¡œì íŠ¸ ì¼ì •ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?'):
    st.session_state.messages.append({'role': 'user', 'content': prompt})
    
    with st.chat_message('user'):
        st.markdown(prompt)
    
    with st.chat_message('assistant'):
        with st.spinner('ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤... ğŸ¤”'):
            try:
                bucket_name = f'notion-chatbot-data-965037532757-ap-northeast-2'
                
                # ì„ íƒëœ ê²€ìƒ‰ ë°©ì‹ì— ë”°ë¼ ë¬¸ì„œ ê²€ìƒ‰
                if search_method == 'S3 í‚¤ì›Œë“œ ê²€ìƒ‰':
                    documents = search_s3_documents(prompt, bucket_name)
                    search_info = f'ğŸ” S3 í‚¤ì›Œë“œ ê²€ìƒ‰ ì‚¬ìš©'
                else:
                    documents = search_opensearch_documents(prompt)
                    search_info = f'ğŸ§  OpenSearch ì˜ë¯¸ ê²€ìƒ‰ ì‚¬ìš©'
                
                st.info(search_info)
                
                if documents:
                    # Bedrockìœ¼ë¡œ RAG ì‘ë‹µ ìƒì„±
                    answer = generate_bedrock_response(prompt, documents)
                    st.markdown(answer)
                    
                    # ì°¸ê³  ë¬¸ì„œ í‘œì‹œ
                    with st.expander('ğŸ“š ì°¸ê³  ë¬¸ì„œ', expanded=True):
                        for i, doc in enumerate(documents[:3], 1):
                            st.markdown(f'**{i}. {doc.get("title", "ë¬¸ì„œ")}**')
                            content_preview = doc.get('content', 'ë‚´ìš© ì—†ìŒ')[:200]
                            st.markdown(f'ë‚´ìš©: {content_preview}...')
                            if doc.get('url'):
                                st.markdown(f'[ğŸ“„ ì›ë³¸ ë³´ê¸°]({doc["url"]})')
                            if doc.get('similarity_score'):
                                st.markdown(f'ìœ ì‚¬ë„ ì ìˆ˜: {doc["similarity_score"]:.3f}')
                            st.markdown('---')
                    
                    # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                    st.session_state.messages.append({
                        'role': 'assistant', 
                        'content': answer,
                        'sources': documents
                    })
                else:
                    no_result_msg = 'ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Notion ë°ì´í„°ê°€ ë™ê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.'
                    st.markdown(no_result_msg)
                    st.session_state.messages.append({'role': 'assistant', 'content': no_result_msg})
                    
            except Exception as e:
                error_msg = f'ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
                st.error(error_msg)
                st.session_state.messages.append({'role': 'assistant', 'content': error_msg})

st.markdown('---')
st.markdown('<div style="text-align: center; color: #666; padding: 1rem;"><p>ğŸ” S3 í‚¤ì›Œë“œ ê²€ìƒ‰ vs ğŸ§  OpenSearch ì˜ë¯¸ ê²€ìƒ‰ ë¹„êµ ë°ëª¨</p><p>ğŸ“š ë‘ ê°€ì§€ ê²€ìƒ‰ ë°©ì‹ì˜ ì°¨ì´ë¥¼ ì§ì ‘ ì²´í—˜í•´ë³´ì„¸ìš”</p><p>ğŸ”„ 1ì‹œê°„ë§ˆë‹¤ Notion ë°ì´í„° ìë™ ë™ê¸°í™”</p></div>', unsafe_allow_html=True)
